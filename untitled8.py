# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HHrx5cSLSWo0tNpCsF3PzpoqU28rfI48

Dataset link: https://www.kaggle.com/datasets/algord/fake-news
"""

!pip install pyTsetlinMachine
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from pyTsetlinMachine.tm import MultiClassTsetlinMachine

file_path = '/content/FakeNewsNet.csv'

df = pd.read_csv(file_path)

print("DataFrame Information:")
print(df.info())

print("\nFirst Row of the DataFrame:")
print(df.iloc[0])

random_articles = df.sample(n=10)

for index, row in random_articles.iterrows():
    print(f"Title: {row['title']}")
    print(f"News URL: {row['news_url']}")
    print(f"Source Domain: {row['source_domain']}")
    print(f"True/False: {'True' if row['real'] == 1 else 'False'}")  # Assuming 'real' column indicates true or false
    print("------")

print("Missing values in each column:")
print(df.isnull().sum())

# Handling missing values
df.dropna(inplace=True)

print("\nMissing values after handling:")
print(df.isnull().sum())

fake_real_count = df['real'].value_counts()

total_news = len(df)
fake_ratio = fake_real_count[0] / total_news
real_ratio = fake_real_count[1] / total_news

# Plotting the bar graph
plt.figure(figsize=(10, 6))
sns.barplot(x=fake_real_count.index, y=fake_real_count.values, palette="vlag")
plt.title('Distribution and Ratio of Fake and Real News')
plt.xlabel('News Type (0 = Fake, 1 = Real)')
plt.ylabel('Count')
plt.xticks([0, 1], ['Fake (Ratio: {:.2f})'.format(fake_ratio), 'Real (Ratio: {:.2f})'.format(real_ratio)])

# Show the plot
plt.show()

df.drop_duplicates(subset='title', inplace=True)

print("Dataset information after removing duplicates:")
print(df.info())
print("Missing values in each column before handling:")
print(df.isnull().sum())

df.dropna(inplace=True)

print("\nMissing values after handling:")
print(df.isnull().sum())

# Save the cleaned dataset to a new file (e.g., CleanedDataset.csv)
df.to_csv('CleanedDataset.csv', index=False)

df = pd.read_csv('/content/CleanedDataset.csv')

fake_news = df[df['real'] == 0]
real_news = df[df['real'] == 1]

# Determine the number of samples to take from each category
desired_samples = min(len(fake_news), len(real_news))

# Randomly sample from each category
sampled_fake_news = fake_news.sample(n=desired_samples, random_state=42)
sampled_real_news = real_news.sample(n=desired_samples, random_state=42)

# Combine the sampled data to create a balanced dataset
balanced_dataset = pd.concat([sampled_fake_news, sampled_real_news])

# Save the balanced dataset to a new CSV file
balanced_dataset.to_csv('balanced_fake_real_news_dataset.csv', index=False)

print(f"Balanced dataset saved as 'balanced_fake_real_news_dataset.csv' with {desired_samples} samples for each category.")

balanced_dataset = pd.read_csv('balanced_fake_real_news_dataset.csv')

# Count the occurrences of each category (0 for fake news and 1 for real news)
category_counts = balanced_dataset['real'].value_counts()

# Print the counts
print("Number of 0's (fake news):", category_counts[0])
print("Number of 1's (real news):", category_counts[1])

balanced_dataset = pd.read_csv('balanced_fake_real_news_dataset.csv')

X = balanced_dataset['title']
y = balanced_dataset['real']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Vectorizing the text data
vectorizer = CountVectorizer(stop_words='english', max_features=5000, binary=True)
X_train_bow = vectorizer.fit_transform(X_train)
X_test_bow = vectorizer.transform(X_test)

# Initialize the Tsetlin Machine with adjusted hyperparameters
tm = MultiClassTsetlinMachine(number_of_clauses=100, T=15, s=3.9)

# Train the TM on the BoW vectorized training data
tm.fit(X_train_bow.toarray(), y_train)

# Predictions
y_pred_sparse = tm.predict(X_test_bow.toarray())
y_pred = np.asarray(y_pred_sparse).flatten()

# Evaluate the performance of the TM
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print("Confusion Matrix:")
print(conf_matrix)

balanced_dataset = pd.read_csv('balanced_fake_real_news_dataset.csv')

X = balanced_dataset['title']
y = balanced_dataset['real']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Vectorizing the text data
vectorizer = CountVectorizer(stop_words='english', max_features=5000)
X_train_bow = vectorizer.fit_transform(X_train)
X_test_bow = vectorizer.transform(X_test)

# Initialize the Naive Bayes classifier
nb_classifier = MultinomialNB()

# Train the classifier on the vectorized training data
nb_classifier.fit(X_train_bow, y_train)

# Predictions
y_pred = nb_classifier.predict(X_test_bow)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print("Confusion Matrix:")
print(conf_matrix)